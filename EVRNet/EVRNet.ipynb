{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOCwarLj3kCNyJ1SqI0LJEW"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["!pip install pytorch_msssim"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9xjS2pm7JCY5","executionInfo":{"status":"ok","timestamp":1723643300941,"user_tz":-330,"elapsed":83673,"user":{"displayName":"Arnav Malhotra","userId":"01875622780405960400"}},"outputId":"6bd255b0-0a68-455e-d633-f130a9a8fe2b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting pytorch_msssim\n","  Downloading pytorch_msssim-1.0.0-py3-none-any.whl.metadata (8.0 kB)\n","Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from pytorch_msssim) (2.3.1+cu121)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->pytorch_msssim) (3.15.4)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->pytorch_msssim) (4.12.2)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->pytorch_msssim) (1.13.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->pytorch_msssim) (3.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->pytorch_msssim) (3.1.4)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->pytorch_msssim) (2024.6.1)\n","Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch->pytorch_msssim)\n","  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch->pytorch_msssim)\n","  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch->pytorch_msssim)\n","  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch->pytorch_msssim)\n","  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cublas-cu12==12.1.3.1 (from torch->pytorch_msssim)\n","  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cufft-cu12==11.0.2.54 (from torch->pytorch_msssim)\n","  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-curand-cu12==10.3.2.106 (from torch->pytorch_msssim)\n","  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch->pytorch_msssim)\n","  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch->pytorch_msssim)\n","  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-nccl-cu12==2.20.5 (from torch->pytorch_msssim)\n","  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n","Collecting nvidia-nvtx-cu12==12.1.105 (from torch->pytorch_msssim)\n","  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n","Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch->pytorch_msssim) (2.3.1)\n","Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch->pytorch_msssim)\n","  Using cached nvidia_nvjitlink_cu12-12.6.20-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->pytorch_msssim) (2.1.5)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->pytorch_msssim) (1.3.0)\n","Downloading pytorch_msssim-1.0.0-py3-none-any.whl (7.7 kB)\n","Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n","Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n","Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n","Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n","Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n","Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n","Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n","Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n","Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n","Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n","Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n","Using cached nvidia_nvjitlink_cu12-12.6.20-py3-none-manylinux2014_x86_64.whl (19.7 MB)\n","Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, pytorch_msssim\n","Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.6.20 nvidia-nvtx-cu12-12.1.105 pytorch_msssim-1.0.0\n"]}]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0UAT7eQ-EKrJ","executionInfo":{"status":"ok","timestamp":1723642001415,"user_tz":-330,"elapsed":23589,"user":{"displayName":"Arnav Malhotra","userId":"01875622780405960400"}},"outputId":"7a4ee4ab-709e-4336-86e7-76e17dc616e6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","from torchvision import transforms\n","from PIL import Image\n","\n","def apply_1x1_convolution(image_tensor, in_channels, out_channels):\n","    \"\"\"\n","    Applies a 1x1 convolution to an image tensor.\n","\n","    Parameters:\n","        image_tensor (torch.Tensor): Input image tensor with shape (N, C, H, W)\n","        in_channels (int): Number of input channels in the image tensor\n","        out_channels (int): Number of output channels (feature maps) after convolution\n","\n","    Returns:\n","        torch.Tensor: Output feature map after 1x1 convolution\n","    \"\"\"\n","    if image_tensor.ndim != 4:\n","        raise ValueError(\"Input tensor must have 4 dimensions (N, C, H, W)\")\n","\n","    conv1x1 = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=1)\n","    feature_map = conv1x1(image_tensor)\n","\n","    return feature_map\n","\n","def load_and_preprocess_image(image_path):\n","    \"\"\"\n","    Loads and preprocesses an image without resizing.\n","\n","    Parameters:\n","        image_path (str): Path to the image file\n","\n","    Returns:\n","        torch.Tensor: Preprocessed image tensor\n","    \"\"\"\n","    image = Image.open(image_path).convert('RGB')\n","\n","    transform = transforms.Compose([\n","        transforms.ToTensor(),          # Convert image to tensor\n","        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize image\n","    ])\n","\n","    image_tensor = transform(image).unsqueeze(0)  # Add batch dimension\n","\n","    return image_tensor\n","\n","def process_images(reference_image_path, current_image_path, in_channels, out_channels):\n","    \"\"\"\n","    Processes two images by applying 1x1 convolution to the reference image, and concatenates all three.\n","\n","    Parameters:\n","        reference_image_path (str): Path to the reference image file\n","        current_image_path (str): Path to the current image file\n","        in_channels (int): Number of input channels in the images\n","        out_channels (int): Number of output channels for the convolution\n","\n","    Returns:\n","        torch.Tensor: Concatenated tensor of reference image, current image, and feature map\n","    \"\"\"\n","    # Load and preprocess images\n","    reference_image_tensor = load_and_preprocess_image(reference_image_path)\n","    current_image_tensor = load_and_preprocess_image(current_image_path)\n","\n","    print(f\"Reference image tensor shape: {reference_image_tensor.shape}\")\n","    print(f\"Current image tensor shape: {current_image_tensor.shape}\")\n","\n","    # Apply 1x1 convolution to reference image\n","    feature_map = apply_1x1_convolution(reference_image_tensor, in_channels, out_channels)\n","\n","    print(f\"Feature map shape: {feature_map.shape}\")\n","\n","    # Concatenate reference image, current image, and feature map along the channel dimension\n","    concatenated_tensor = torch.cat((reference_image_tensor, current_image_tensor, feature_map), dim=1)\n","\n","    print(f\"Concatenated tensor shape: {concatenated_tensor.shape}\")\n","\n","    return concatenated_tensor\n","\n","# Example usage\n","reference_image_path = '/content/drive/MyDrive/ImageSuperResolutionData/image_0.png'  # Replace with your reference image path\n","current_image_path = '/content/drive/MyDrive/ImageSuperResolutionData/image_1.png'      # Replace with your current image path\n","\n","# Set number of input channels (3 for RGB images) and output channels for the convolution\n","in_channels = 3\n","out_channels = 10\n","\n","# Process images and get the concatenated tensor\n","concatenated_tensor = process_images(reference_image_path, current_image_path, in_channels, out_channels)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QrDplX51ulXh","executionInfo":{"status":"ok","timestamp":1723653619419,"user_tz":-330,"elapsed":434,"user":{"displayName":"Arnav Malhotra","userId":"01875622780405960400"}},"outputId":"5765f279-3bc6-41af-91c4-7e57d33478c9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Reference image tensor shape: torch.Size([1, 3, 256, 256])\n","Current image tensor shape: torch.Size([1, 3, 256, 256])\n","Feature map shape: torch.Size([1, 10, 256, 256])\n","Concatenated tensor shape: torch.Size([1, 16, 256, 256])\n"]}]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torchvision import transforms\n","from PIL import Image\n","import os\n","\n","class ConvolutionalUnit(nn.Module):\n","    def __init__(self, in_channels, out_channels, use_multi_scale=True):\n","        super(ConvolutionalUnit, self).__init__()\n","\n","        if use_multi_scale:\n","            self.single_scale_conv = None\n","            self.multi_scale_convs = nn.ModuleList([\n","                nn.Conv2d(in_channels, in_channels, kernel_size=7, padding=3, groups=in_channels),\n","                nn.Conv2d(in_channels, in_channels, kernel_size=5, padding=2, groups=in_channels),\n","                nn.Conv2d(in_channels, in_channels, kernel_size=3, padding=1, groups=in_channels)\n","            ])\n","        else:\n","            self.single_scale_conv = nn.Conv2d(in_channels, in_channels, kernel_size=7, padding=3, groups=in_channels)\n","            self.multi_scale_convs = None\n","\n","        self.global_avg_pool = nn.AdaptiveAvgPool2d(1)\n","        self.fc1 = nn.Linear(in_channels, in_channels // 16)\n","        self.relu = nn.ReLU(inplace=True)\n","        self.fc2 = nn.Linear(in_channels // 16, in_channels)\n","        self.sigmoid = nn.Sigmoid()\n","        self.conv1x1 = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n","\n","    def forward(self, x):\n","        if self.multi_scale_convs:\n","            conv_results = [conv(x) for conv in self.multi_scale_convs]\n","            x = sum(conv_results)\n","        else:\n","            x = self.single_scale_conv(x)\n","\n","        se = self.global_avg_pool(x)\n","        se = se.view(se.size(0), -1)\n","        se = self.fc1(se)\n","        se = self.relu(se)\n","        se = self.fc2(se)\n","        se = self.sigmoid(se)\n","        se = se.view(se.size(0), se.size(1), 1, 1)\n","        x = x * se\n","\n","        x = self.conv1x1(x)\n","        return x\n","\n","\n","class SmallEncoderDecoderNet(nn.Module):\n","    def __init__(self, in_channels, out_channels, use_multi_scale=True):\n","        super(SmallEncoderDecoderNet, self).__init__()\n","\n","        self.encoder_conv1 = nn.Conv2d(in_channels, 64, kernel_size=5, padding=2)\n","        self.encoder_conv2 = nn.Conv2d(64, 128, kernel_size=5, stride=2, padding=2)\n","        self.encoder_conv3 = nn.Conv2d(128, 256, kernel_size=1)\n","\n","        self.convolutional_unit = ConvolutionalUnit(256, 256, use_multi_scale=use_multi_scale)\n","\n","        self.conv_before_shuffle = nn.Conv2d(256, 256, kernel_size=1)\n","        self.pixel_shuffle = nn.PixelShuffle(upscale_factor=2)\n","        self.decoder_conv1x1 = nn.Conv2d(256 // 4, out_channels, kernel_size=1)\n","\n","    def forward(self, x):\n","        x = F.relu(self.encoder_conv1(x))\n","        x = F.relu(self.encoder_conv2(x))\n","        x = F.relu(self.encoder_conv3(x))\n","\n","        x = self.convolutional_unit(x)\n","\n","        x = self.conv_before_shuffle(x)\n","        x = self.pixel_shuffle(x)\n","        x = self.decoder_conv1x1(x)\n","\n","        return x\n","\n","\n","class ImageProcessor:\n","    def __init__(self, model, in_channels=3, intermediate_out_channels=10, final_out_channels=64):\n","        self.model = model\n","        self.in_channels = in_channels\n","        self.intermediate_out_channels = intermediate_out_channels\n","        self.final_out_channels = final_out_channels\n","\n","    def load_and_preprocess_image(self, image_path):\n","        image = Image.open(image_path).convert('RGB')\n","        transform = transforms.Compose([\n","            transforms.ToTensor(),\n","            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","        ])\n","        return transform(image).unsqueeze(0)  # Add batch dimension\n","\n","    def apply_1x1_convolution(self, image_tensor):\n","        conv1x1 = nn.Conv2d(self.in_channels, self.intermediate_out_channels, kernel_size=1)\n","        return conv1x1(image_tensor)\n","\n","    def process_images(self, reference_image_tensor, current_image_tensor):\n","        feature_map = self.apply_1x1_convolution(reference_image_tensor)\n","        concatenated_tensor = torch.cat((reference_image_tensor, current_image_tensor, feature_map), dim=1)\n","        return concatenated_tensor\n","\n","    def process_burst_images(self, folder_path):\n","        image_files = sorted([f for f in os.listdir(folder_path) if f.endswith('.png') or f.endswith('.jpg') or f.endswith('.jpeg')])\n","\n","        reference_image_path = os.path.join(folder_path, image_files[0])\n","        reference_image_tensor = self.load_and_preprocess_image(reference_image_path)\n","\n","        output_tensors = []\n","\n","        for current_image_filename in image_files[1:]:\n","            current_image_path = os.path.join(folder_path, current_image_filename)\n","            current_image_tensor = self.load_and_preprocess_image(current_image_path)\n","            concatenated_tensor = self.process_images(reference_image_tensor, current_image_tensor)\n","            output_tensor = self.model(concatenated_tensor)\n","            output_tensors.append(output_tensor.squeeze(0))\n","\n","        stacked_output_tensors = torch.stack(output_tensors)\n","        return output_tensors, stacked_output_tensors\n","\n","\n","class BurstImagePipeline:\n","    def __init__(self, folder_path, model, in_channels=3, intermediate_out_channels=10, final_out_channels=64):\n","        self.folder_path = folder_path\n","        self.processor = ImageProcessor(model, in_channels, intermediate_out_channels, final_out_channels)\n","\n","    def run(self):\n","        return self.processor.process_burst_images(self.folder_path)\n","\n","\n","# Example usage\n","folder_path = '/content/drive/MyDrive/ImageSuperResolutionData/'\n","model = SmallEncoderDecoderNet(in_channels=16, out_channels=64, use_multi_scale=True)\n","\n","pipeline = BurstImagePipeline(folder_path, model)\n","output_tensors, stacked_output_tensors = pipeline.run()\n","\n","print(f\"Stacked output tensor shape: {stacked_output_tensors.shape}\")  # Should be [13, 64, 256, 256]\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uBAaJlf2yYpg","executionInfo":{"status":"ok","timestamp":1723654608596,"user_tz":-330,"elapsed":6271,"user":{"displayName":"Arnav Malhotra","userId":"01875622780405960400"}},"outputId":"f52b8679-99cc-47c7-e03e-38870cf12236"},"execution_count":64,"outputs":[{"output_type":"stream","name":"stdout","text":["Stacked output tensor shape: torch.Size([13, 64, 256, 256])\n"]}]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import pytorch_lightning as pl\n","from pytorch_lightning import seed_everything\n","from utils.metrics import PSNR\n","\n","psnr_fn = PSNR(boundary_ignore=40)\n","seed_everything(13)\n","\n","##############################################################################################\n","######################### Residual Global Context Attention Block ##########################################\n","##############################################################################################\n","\n","class RGCAB(nn.Module):\n","    def __init__(self, num_features, num_rcab, reduction):\n","        super(RGCAB, self).__init__()\n","        self.module = [RGCA(num_features, reduction) for _ in range(num_rcab)]\n","        self.module.append(nn.Conv2d(num_features, num_features, kernel_size=3, padding=1, bias=False))\n","        self.module = nn.Sequential(*self.module)\n","\n","    def forward(self, x):\n","        return x + self.module(x)\n","\n","class RGCA(nn.Module):\n","    def __init__(self, n_feat, reduction=8, bias=False, act=nn.LeakyReLU(negative_slope=0.2,inplace=True), groups =1):\n","        super(RGCA, self).__init__()\n","\n","        self.n_feat = n_feat\n","        self.groups = groups\n","        self.reduction = reduction\n","\n","        modules_body = [nn.Conv2d(n_feat, n_feat, 3,1,1 , bias=bias, groups=groups), act, nn.Conv2d(n_feat, n_feat, 3,1,1 , bias=bias, groups=groups)]\n","        self.body = nn.Sequential(*modules_body)\n","\n","        self.gcnet = nn.Sequential(GCA(n_feat, n_feat))\n","        self.conv1x1 = nn.Conv2d(n_feat, n_feat, kernel_size=1, bias=bias)\n","\n","    def forward(self, x):\n","        res = self.body(x)\n","        res = self.gcnet(res)\n","        res = self.conv1x1(res)\n","        res += x\n","        return res\n","\n","######################### Global Context Attention ##########################################\n","\n","class GCA(nn.Module):\n","    def __init__(self, inplanes, planes, act=nn.LeakyReLU(negative_slope=0.2,inplace=True), bias=False):\n","        super(GCA, self).__init__()\n","\n","        self.conv_mask = nn.Conv2d(inplanes, 1, kernel_size=1, bias=bias)\n","        self.softmax = nn.Softmax(dim=2)\n","\n","        self.channel_add_conv = nn.Sequential(\n","            nn.Conv2d(inplanes, planes, kernel_size=1, bias=bias),\n","            act,\n","            nn.Conv2d(planes, inplanes, kernel_size=1, bias=bias)\n","        )\n","\n","    def spatial_pool(self, x):\n","        batch, channel, height, width = x.size()\n","        input_x = x.view(batch, channel, height * width)\n","        input_x = input_x.unsqueeze(1)\n","        context_mask = self.conv_mask(x).view(batch, 1, height * width)\n","        context_mask = self.softmax(context_mask).unsqueeze(3)\n","        context = torch.matmul(input_x, context_mask).view(batch, channel, 1, 1)\n","        return context\n","\n","    def forward(self, x):\n","        context = self.spatial_pool(x)\n","        channel_add_term = self.channel_add_conv(context)\n","        x = x + channel_add_term\n","        return x\n","\n","##############################################################################################\n","######################### Multi-scale Feature Extractor ##########################################\n","##############################################################################################\n","\n","class UpSample(nn.Module):\n","    def __init__(self, in_channels, chan_factor, bias=False):\n","        super(UpSample, self).__init__()\n","        self.up = nn.Sequential(\n","            nn.Conv2d(in_channels, int(in_channels/chan_factor), 1, stride=1, padding=0, bias=bias),\n","            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n","        )\n","\n","    def forward(self, x):\n","        return self.up(x)\n","\n","class DownSample(nn.Module):\n","    def __init__(self, in_channels, chan_factor, bias=False):\n","        super(DownSample, self).__init__()\n","        self.down = nn.Sequential(\n","            nn.AvgPool2d(2, ceil_mode=True, count_include_pad=False),\n","            nn.Conv2d(in_channels, int(in_channels*chan_factor), 1, stride=1, bias=bias)\n","        )\n","\n","    def forward(self, x):\n","        return self.down(x)\n","\n","class MSF(nn.Module):\n","    def __init__(self, in_channels=64, reduction=8, bias=False):\n","        super(MSF, self).__init__()\n","\n","        self.feat_ext1 = nn.Sequential(*[RGCAB(in_channels, 2, reduction) for _ in range(2)])\n","\n","        self.down1 = DownSample(in_channels, chan_factor=1.5)\n","        self.feat_ext2 = nn.Sequential(*[RGCAB(int(in_channels*1.5), 2, reduction) for _ in range(2)])\n","\n","        self.down2 = DownSample(int(in_channels*1.5), chan_factor=1.5)\n","        self.feat_ext3 = nn.Sequential(*[RGCAB(int(in_channels*1.5*1.5), 2, reduction) for _ in range(1)])\n","\n","        self.up2 = UpSample(int(in_channels*1.5*1.5), chan_factor=1.5)\n","        self.feat_ext5 = nn.Sequential(*[RGCAB(int(in_channels*1.5), 2, reduction) for _ in range(2)])\n","\n","        self.up1 = UpSample(int(in_channels*1.5), chan_factor=1.5)\n","        self.feat_ext6 = nn.Sequential(*[RGCAB(in_channels, 2, reduction) for _ in range(2)])\n","\n","    def forward(self, x):\n","        x = self.feat_ext1(x)\n","\n","        enc_1 = self.down1(x)\n","        enc_1 = self.feat_ext2(enc_1)\n","\n","        enc_2 = self.down2(enc_1)\n","        enc_2 = self.feat_ext3(enc_2)\n","\n","        dec_2 = self.up2(enc_2)\n","        dec_2 = self.feat_ext5(dec_2 + enc_1)\n","\n","        dec_1 = self.up1(dec_2)\n","        dec_2 = self.feat_ext6(dec_1 + x)\n","\n","        return dec_2\n","\n","##############################################################################################\n","######################### Adaptive Group Up-sampling Module ##########################################\n","##############################################################################################\n","\n","class AGU(nn.Module):\n","    def __init__(self, in_channels, height, reduction=8, bias=False):\n","        super(AGU, self).__init__()\n","\n","        self.height = height\n","        d = max(int(in_channels/reduction), 4)\n","\n","        self.conv_du = nn.Sequential(\n","            nn.Conv2d(in_channels, d, 1, padding=0, bias=bias),\n","            nn.LeakyReLU(negative_slope=0.2, inplace=True)\n","        )\n","\n","        self.convs = nn.ModuleList([nn.Conv2d(d, in_channels, kernel_size=1, stride=1, bias=bias) for _ in range(self.height)])\n","\n","        self.softmax = nn.Softmax(dim=1)\n","        self.up = nn.ConvTranspose2d(in_channels*4, in_channels, 3, stride=2, padding=1, output_padding=1, bias=bias)\n","\n","    def forward(self, inp_feats):\n","        batch_size, b, n_feats, H, W = inp_feats.size()\n","\n","        feats_U = torch.sum(inp_feats, dim=1)\n","        feats_Z = self.conv_du(feats_U)\n","\n","        dense_attention = [conv(feats_Z) for conv in self.convs]\n","        dense_attention = torch.cat(dense_attention, dim=1)\n","\n","        dense_attention = dense_attention.view(batch_size, self.height, n_feats, H, W)\n","        dense_attention = self.softmax(dense_attention)\n","\n","        feats_V = inp_feats * dense_attention\n","        feats_V = feats_V.view(batch_size, -1, H, W)\n","        feats_V = self.up(feats_V)\n","\n","        return feats_V\n","\n","##############################################################################################\n","######################### Burst Image Processing Network (BIPNet) ##########################################\n","##############################################################################################\n","\n","# Custom Convolutional Unit Class Definition\n","class ConvolutionalUnit(nn.Module):\n","    def __init__(self, in_channels, out_channels, use_multi_scale=True):\n","        super(ConvolutionalUnit, self).__init__()\n","        if use_multi_scale:\n","            self.single_scale_conv = None\n","            self.multi_scale_convs = nn.ModuleList([\n","                nn.Conv2d(in_channels, in_channels, kernel_size=7, padding=3, groups=in_channels),\n","                nn.Conv2d(in_channels, in_channels, kernel_size=5, padding=2, groups=in_channels),\n","                nn.Conv2d(in_channels, in_channels, kernel_size=3, padding=1, groups=in_channels)\n","            ])\n","        else:\n","            self.single_scale_conv = nn.Conv2d(in_channels, in_channels, kernel_size=7, padding=3, groups=in_channels)\n","            self.multi_scale_convs = None\n","\n","        self.global_avg_pool = nn.AdaptiveAvgPool2d(1)\n","        self.fc1 = nn.Linear(in_channels, in_channels // 16)\n","        self.relu = nn.ReLU(inplace=True)\n","        self.fc2 = nn.Linear(in_channels // 16, in_channels)\n","        self.sigmoid = nn.Sigmoid()\n","        self.conv1x1 = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n","\n","    def forward(self, x):\n","        if self.multi_scale_convs:\n","            conv_results = [conv(x) for conv in self.multi_scale_convs]\n","            x = sum(conv_results)\n","        else:\n","            x = self.single_scale_conv(x)\n","\n","        se = self.global_avg_pool(x)\n","        se = se.view(se.size(0), -1)\n","        se = self.fc1(se)\n","        se = self.relu(se)\n","        se = self.fc2(se)\n","        se = self.sigmoid(se)\n","        se = se.view(se.size(0), se.size(1), 1, 1)\n","        x = x * se\n","\n","        x = self.conv1x1(x)\n","        return x\n","\n","# Custom Small Encoder-Decoder Network Class Definition\n","class SmallEncoderDecoderNet(nn.Module):\n","    def __init__(self, in_channels, out_channels, use_multi_scale=True):\n","        super(SmallEncoderDecoderNet, self).__init__()\n","        self.encoder_conv1 = nn.Conv2d(in_channels, 64, kernel_size=5, padding=2)\n","        self.encoder_conv2 = nn.Conv2d(64, 128, kernel_size=5, stride=2, padding=2)\n","        self.encoder_conv3 = nn.Conv2d(128, 256, kernel_size=1)\n","\n","        self.convolutional_unit = ConvolutionalUnit(256, 256, use_multi_scale=use_multi_scale)\n","\n","        self.conv_before_shuffle = nn.Conv2d(256, 256, kernel_size=1)\n","        self.pixel_shuffle = nn.PixelShuffle(upscale_factor=2)\n","        self.decoder_conv1x1 = nn.Conv2d(256 // 4, out_channels, kernel_size=1)\n","\n","    def forward(self, x):\n","        x = F.relu(self.encoder_conv1(x))\n","        x = F.relu(self.encoder_conv2(x))\n","        x = F.relu(self.encoder_conv3(x))\n","\n","        x = self.convolutional_unit(x)\n","\n","        x = self.conv_before_shuffle(x)\n","        x = self.pixel_shuffle(x)\n","        x = self.decoder_conv1x1(x)\n","\n","        return x\n","\n","# Custom Image Processor for Burst Alignment\n","class ImageProcessor:\n","    def __init__(self, model, in_channels=3, intermediate_out_channels=10, final_out_channels=64):\n","        self.model = model\n","        self.in_channels = in_channels\n","        self.intermediate_out_channels = intermediate_out_channels\n","        self.final_out_channels = final_out_channels\n","\n","    def load_and_preprocess_image(self, image):\n","        transform = transforms.Compose([\n","            transforms.ToTensor(),\n","            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","        ])\n","        return transform(image).unsqueeze(0)  # Add batch dimension\n","\n","    def apply_1x1_convolution(self, image_tensor):\n","        conv1x1 = nn.Conv2d(self.in_channels, self.intermediate_out_channels, kernel_size=1)\n","        return conv1x1(image_tensor)\n","\n","    def process_images(self, reference_image_tensor, current_image_tensor):\n","        feature_map = self.apply_1x1_convolution(reference_image_tensor)\n","        concatenated_tensor = torch.cat((reference_image_tensor, current_image_tensor, feature_map), dim=1)\n","        return concatenated_tensor\n","\n","    def process_burst_images(self, burst):\n","        reference_image_tensor = burst[0]\n","\n","        output_tensors = []\n","\n","        for current_image_tensor in burst[1:]:\n","            concatenated_tensor = self.process_images(reference_image_tensor, current_image_tensor)\n","            output_tensor = self.model(concatenated_tensor)\n","            output_tensors.append(output_tensor.squeeze(0))\n","\n","        stacked_output_tensors = torch.stack(output_tensors)\n","        return stacked_output_tensors\n","\n","# Custom Burst Image Pipeline\n","class BurstImagePipeline:\n","    def __init__(self, model, in_channels=3, intermediate_out_channels=10, final_out_channels=64):\n","        self.processor = ImageProcessor(model, in_channels, intermediate_out_channels, final_out_channels)\n","\n","    def run(self, burst):\n","        return self.processor.process_burst_images(burst)\n","\n","# Modified BIPNet Class\n","class BIPNet(pl.LightningModule):\n","    def __init__(self, num_features=64, burst_size=14, reduction=8, bias=False):\n","        super(BIPNet, self).__init__()\n","\n","        self.train_loss = nn.L1Loss()\n","        self.valid_psnr = PSNR(boundary_ignore=40)\n","\n","        self.conv1 = nn.Sequential(nn.Conv2d(4, num_features, kernel_size=3, padding=1, bias=bias))\n","\n","        ####### Custom Burst Alignment Replacement\n","        self.small_encoder_decoder_net = SmallEncoderDecoderNet(in_channels=16, out_channels=num_features, use_multi_scale=True)\n","        self.alignment_pipeline = BurstImagePipeline(self.small_encoder_decoder_net)\n","\n","        ## Feature Processing Module\n","        self.encoder = nn.Sequential(*[RGCAB(num_features, 3, reduction) for _ in range(3)])\n","\n","        ####### Pseudo Burst Feature Fusion\n","        self.conv2 = nn.Sequential(nn.Conv2d(burst_size, num_features, kernel_size=3, padding=1, bias=bias))\n","\n","        ## Multi-scale Feature Extraction\n","        self.UNet = nn.Sequential(MSF(num_features))\n","\n","        ####### Adaptive Group Up-sampling\n","        self.SKFF1 = AGU(num_features, 4)\n","        self.SKFF2 = AGU(num_features, 4)\n","        self.SKFF3 = AGU(num_features, 4)\n","\n","        ## Output Convolution\n","        self.conv3 = nn.Sequential(nn.Conv2d(num_features, 3, kernel_size=3, padding=1, bias=bias))\n","\n","    def forward(self, burst):\n","\n","        ###################\n","        # Input: (B, 4, H/2, W/2)\n","        # Output: (1, 3, 4H, 4W)\n","        ###################\n","\n","        burst = burst[0]\n","        burst_feat = self.conv1(burst)  # (B, num_features, H/2, W/2)\n","\n","        ##################################################\n","        ####### Custom Burst Feature Alignment ################\n","        ##################################################\n","        aligned_burst_feat = self.alignment_pipeline.run(burst_feat)\n","\n","        ##################################################\n","        ####### Edge Boosting Feature Alignment ################\n","        ##################################################\n","\n","        base_frame_feat = burst_feat[0].unsqueeze(0)\n","        burst_feat = self.encoder(aligned_burst_feat)\n","\n","        ## Refined Aligned Feature\n","        burst_feat = self.feat_ext1(burst_feat)\n","        Residual = burst_feat - base_frame_feat\n","        Residual = self.cor_conv1(Residual)\n","        burst_feat += Residual  # (B, num_features, H/2, W/2)\n","\n","        ##################################################\n","        ####### Pseudo Burst Feature Fusion ####################\n","        ##################################################\n","        burst_feat = burst_feat.permute(1, 0, 2, 3).contiguous()\n","        burst_feat = self.conv2(burst_feat)  # (num_features, num_features, H/2, W/2)\n","\n","        ## Multi-scale Feature Extraction\n","        burst_feat = self.UNet(burst_feat)  # (num_features, num_features, H/2, W/2)\n","\n","\n","        ##################################################\n","        ####### Adaptive Group Up-sampling #####################\n","        ##################################################\n","        b, f, H, W = burst_feat.size()\n","        burst_feat = burst_feat.view(b // 4, 4, f, H, W)          # (num_features//4, 4, num_features, H/2, W/2)\n","        burst_feat = self.SKFF1(burst_feat)                     # (num_features//4, num_features, H, W)\n","\n","        b, f, H, W = burst_feat.size()\n","        burst_feat = burst_feat.view(b // 4, 4, f, H, W)          # (num_features//16, 4, num_features, H, W)\n","        burst_feat = self.SKFF2(burst_feat)                     # (num_features//16, num_features, 2H, 2W)\n","\n","        b, f, H, W = burst_feat.size()\n","        burst_feat = burst_feat.view(b // 4, 4, f, H, W)          # (1, 4, num_features, H, W)\n","        burst_feat = self.SKFF3(burst_feat)                     # (1, num_features, 4H, 4W)\n","\n","        ## Output Convolution\n","        burst_feat = self.conv3(burst_feat)                     # (1, 3, 4H, 4W)\n","\n","        return burst_feat\n","\n","    def training_step(self, train_batch, batch_idx):\n","        x, y, flow_vectors, meta_info = train_batch\n","        pred = self.forward(x)\n","        pred = pred.clamp(0.0, 1.0)\n","        loss = self.train_loss(pred, y)\n","        self.log('train_loss', loss, on_step=True, on_epoch=True)\n","        return loss\n","\n","    def validation_step(self, val_batch, batch_idx):\n","        x, y, flow_vectors, meta_info = val_batch\n","        pred = self.forward(x)\n","        pred = pred.clamp(0.0, 1.0)\n","        PSNR = self.valid_psnr(pred, y)\n","        return PSNR\n","\n","    def validation_epoch_end(self, outs):\n","        PSNR = torch.stack(outs).mean()\n","        self.log('val_psnr', PSNR, on_step=False, on_epoch=True, prog_bar=True)\n","\n","    def configure_optimizers(self):\n","        optimizer = torch.optim.AdamW(self.parameters(), lr=1e-4)\n","        lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, 300, eta_min=1e-6)\n","        return [optimizer], [lr_scheduler]\n","\n","    def optimizer_zero_grad(self, epoch, batch_idx, optimizer, optimizer_idx):\n","        optimizer.zero_grad(set_to_none=True)\n"],"metadata":{"id":"_9WE8zUB2LQP"},"execution_count":null,"outputs":[]}]}